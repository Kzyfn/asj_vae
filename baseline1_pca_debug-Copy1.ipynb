{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "#sys.path.remove('/usr/local/lib/python3.7/site-packages')\n",
    "sys.path.append('/usr/local/.pyenv/versions/3.6.0/lib/python3.6/site-packages')\n",
    "sys.path.append('/Users/kazuya_yufune/.pyenv/versions/3.6.0/lib/python3.6/site-packages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import expanduser, join\n",
    "\n",
    "\n",
    "import sys\n",
    "\n",
    "import time\n",
    "\n",
    "from nnmnkwii.datasets import FileDataSource, FileSourceDataset\n",
    "from nnmnkwii.datasets import PaddedFileSourceDataset, MemoryCacheDataset  # これはなに？\n",
    "from nnmnkwii.preprocessing import trim_zeros_frames, remove_zeros_frames\n",
    "from nnmnkwii.preprocessing import minmax, meanvar, minmax_scale, scale\n",
    "from nnmnkwii import paramgen\n",
    "from nnmnkwii.io import hts\n",
    "from nnmnkwii.frontend import merlin as fe\n",
    "from nnmnkwii.postfilters import merlin_post_filter\n",
    "\n",
    "from os.path import join, expanduser, basename, splitext, basename, exists\n",
    "import os\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "from scipy.io import wavfile\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pyworld\n",
    "import pysptk\n",
    "import librosa\n",
    "import librosa.display\n",
    "import IPython\n",
    "from IPython.display import Audio\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import expanduser, join\n",
    "\n",
    "\n",
    "import sys\n",
    "\n",
    "import time\n",
    "\n",
    "from nnmnkwii.datasets import FileDataSource, FileSourceDataset\n",
    "from nnmnkwii.datasets import PaddedFileSourceDataset, MemoryCacheDataset  # これはなに？\n",
    "from nnmnkwii.preprocessing import trim_zeros_frames, remove_zeros_frames\n",
    "from nnmnkwii.preprocessing import minmax, meanvar, minmax_scale, scale\n",
    "from nnmnkwii import paramgen\n",
    "from nnmnkwii.io import hts\n",
    "from nnmnkwii.frontend import merlin as fe\n",
    "from nnmnkwii.postfilters import merlin_post_filter\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "from os.path import join, expanduser, basename, splitext, basename, exists\n",
    "import os\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "from scipy.io import wavfile\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pyworld\n",
    "import pysptk\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "\n",
    "DATA_ROOT = \"./data/basic5000\"  # NIT-ATR503/\"#\n",
    "test_size = 0.01  # This means 480 utterances for training data\n",
    "random_state = 1234\n",
    "\n",
    "\n",
    "mgc_dim = 180  # メルケプストラム次数　？？\n",
    "lf0_dim = 3  # 対数fo　？？ なんで次元が３？\n",
    "vuv_dim = 1  # 無声or 有声フラグ　？？\n",
    "bap_dim = 15  # 発話ごと非周期成分　？？\n",
    "\n",
    "duration_linguistic_dim = 438  # question_jp.hed で、ラベルに対する言語特徴量をルールベースで記述してる\n",
    "acoustic_linguisic_dim = 535  # 上のやつ+frame_features とは？？\n",
    "acoustic_linguisic_dim_model = 442 + 8\n",
    "duration_dim = 1\n",
    "acoustic_dim = mgc_dim + lf0_dim + vuv_dim + bap_dim  # aoustice modelで求めたいもの\n",
    "\n",
    "fs = 48000\n",
    "frame_period = 5\n",
    "fftlen = pyworld.get_cheaptrick_fft_size(fs)\n",
    "alpha = pysptk.util.mcepalpha(fs)\n",
    "hop_length = int(0.001 * frame_period * fs)\n",
    "\n",
    "mgc_start_idx = 0\n",
    "lf0_start_idx = 180\n",
    "vuv_start_idx = 183\n",
    "bap_start_idx = 184\n",
    "\n",
    "windows = [\n",
    "    (0, 0, np.array([1.0])),\n",
    "    (1, 1, np.array([-0.5, 0.0, 0.5])),\n",
    "    (1, 1, np.array([1.0, -2.0, 1.0])),\n",
    "]\n",
    "\n",
    "use_phone_alignment = True\n",
    "acoustic_subphone_features = \"coarse_coding\" if use_phone_alignment else \"full\"  # とは？\n",
    "\n",
    "\n",
    "from models import BinaryFileSource\n",
    "\n",
    "\n",
    "X = {\"acoustic\": {}}\n",
    "Y = {\"acoustic\": {}}\n",
    "utt_lengths = {\"acoustic\": {}}\n",
    "for ty in [\"acoustic\"]:\n",
    "    for phase in [\"train\", \"test\"]:\n",
    "        train = phase == \"train\"\n",
    "        x_dim = duration_linguistic_dim if ty == \"duration\" else acoustic_linguisic_dim\n",
    "        y_dim = duration_dim if ty == \"duration\" else acoustic_dim\n",
    "        X[ty][phase] = FileSourceDataset(\n",
    "            BinaryFileSource(join(DATA_ROOT, \"X_{}_with_accent\".format(ty)), dim=x_dim, train=train)\n",
    "        )\n",
    "        Y[ty][phase] = FileSourceDataset(\n",
    "            BinaryFileSource(join(DATA_ROOT, \"Y_{}_with_accent\".format(ty)), dim=y_dim, train=train)\n",
    "        )\n",
    "        utt_lengths[ty][phase] = np.array([len(x) for x in X[ty][phase]], dtype=np.int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_min, X_max, Y_mean, Y_var, Y_scale = {}, {}, {}, {}, {}\n",
    "\n",
    "for typ in [\"acoustic\"]:\n",
    "    X_min[typ], X_max[typ] = minmax(X['acoustic']['train'], utt_lengths[typ][\"train\"])\n",
    "    Y_mean[typ], Y_var[typ] = meanvar(Y[typ][\"train\"], utt_lengths[typ][\"train\"])\n",
    "    Y_scale[typ] = np.sqrt(Y_var[typ])\n",
    "\n",
    "X_acoustic_train = [minmax_scale(x, X_min[\"acoustic\"], X_max[\"acoustic\"], feature_range=(0.01, 0.99)) for x in X['acoustic']['train']]\n",
    "Y_acoustic_train = [\n",
    "    y\n",
    "    for y in Y[\"acoustic\"][\n",
    "        \"train\"\n",
    "    ]  # scale(y, Y_mean[\"acoustic\"], Y_scale[\"acoustic\"]) for y in Y[\"acoustic\"][\"train\"]\n",
    "]\n",
    "\n",
    "\n",
    "X_acoustic_test = [\n",
    "    minmax_scale(x, X_min[\"acoustic\"], X_max[\"acoustic\"], feature_range=(0.01, 0.99))\n",
    "    for x in  X['acoustic']['test']\n",
    "]\n",
    "Y_acoustic_test = [\n",
    "    y\n",
    "    for y in Y[\"acoustic\"][\n",
    "        \"test\"\n",
    "    ]  # scale(y, Y_mean[\"acoustic\"], Y_scale[\"acoustic\"]) for y in Y[\"acoustic\"][\"test\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "accent_info_train_ = [\n",
    "    np.concatenate([x[:, 285:335], x[:, 488:531]], axis=1).reshape(-1, 93)\n",
    "    for x in X_acoustic_train\n",
    "]\n",
    "\n",
    "accent_info_train = []\n",
    "\n",
    "for x in accent_info_train_:\n",
    "    accent_info_train += list(x)\n",
    "\n",
    "accent_info_train = np.array(accent_info_train)\n",
    "\n",
    "pca = PCA(whiten=True)\n",
    "pca.fit(accent_info_train)\n",
    "\n",
    "compressed_accent_info_train = [\n",
    "    pca.fit_transform(np.concatenate([x[:, 285:335], x[:, 488:531]], axis=1))\n",
    "    for x in X_acoustic_train\n",
    "]\n",
    "compressed_accent_info_test = [\n",
    "    pca.fit_transform(np.concatenate([x[:, 285:335], x[:, 488:531]], axis=1))\n",
    "    for x in X_acoustic_test\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.26834843, 0.43708625, 0.54578346, 0.632966  , 0.69899005,\n",
       "       0.75510263, 0.803256  , 0.8474156 , 0.8857547 , 0.9192579 ,\n",
       "       0.94434136, 0.96400046, 0.9745188 , 0.98117435, 0.98744696,\n",
       "       0.99218404, 0.99589837, 0.99857354, 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        ], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.explained_variance_ratio_.cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = []\n",
    "x_test = []\n",
    "\n",
    "for i in range(len(X_acoustic_train)):\n",
    "    x = X_acoustic_train[i]\n",
    "    x_train.append(np.concatenate(\n",
    "        [x[:, :285], x[:, 335:488], x[:, 531:], compressed_accent_info_train[i][:, :8]], axis=1\n",
    "    ))\n",
    "\n",
    "for i in range(len(X_acoustic_test)):\n",
    "    x = X_acoustic_test[i]\n",
    "    x_test.append(np.concatenate(\n",
    "        [x[:, :285], x[:, 335:488], x[:, 531:], compressed_accent_info_test[i][:, :8]], axis=1\n",
    "    ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_loader = [[x, y] for x, y in zip(x_train, Y_acoustic_train)]\n",
    "test_loader = [[x, y] for x, y in zip(x_test, Y_acoustic_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data as data_utils\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from tqdm import tnrange, tqdm\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Rnn(nn.Module):\n",
    "    def __init__(self, bidirectional=True, num_layers=2):\n",
    "        super(Rnn, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.num_direction = 2 if bidirectional else 1\n",
    "        ##ここまでエンコーダ\n",
    "\n",
    "        self.fc11 = nn.Linear(\n",
    "            acoustic_linguisic_dim_model, acoustic_linguisic_dim_model\n",
    "        )\n",
    "\n",
    "        self.lstm2 = nn.LSTM(\n",
    "            acoustic_linguisic_dim_model,\n",
    "            512,\n",
    "            num_layers,\n",
    "            bidirectional=bidirectional,\n",
    "            dropout=0.15,\n",
    "        )\n",
    "        self.fc3 = nn.Linear(self.num_direction * 512, acoustic_dim)\n",
    "\n",
    "    def decode(self, linguistic_features):\n",
    "        x = self.fc11(linguistic_features.view(linguistic_features.size()[0], 1, -1))\n",
    "        x = F.relu(x)\n",
    "        h3, (h, c) = self.lstm2(x)\n",
    "        h3 = F.relu(h3)\n",
    "\n",
    "        return self.fc3(h3)  # torch.sigmoid(self.fc3(h3))\n",
    "\n",
    "    def forward(self, linguistic_features):\n",
    "\n",
    "        return self.decode(linguistic_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Rnn()\n",
    "model.load_state_dict(torch.load('baseline/baseline1_pca8_10.pth', map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recon(index, model=model, train=False):\n",
    "    with torch.no_grad():\n",
    "        x = minmax_scale(x_test[index], X_min[\"acoustic\"], X_max[\"acoustic\"], feature_range=(0.01, 0.99))\n",
    "        if train:\n",
    "            x = minmax_scale(x_train[index], X_min[\"acoustic\"], X_max[\"acoustic\"], feature_range=(0.01, 0.99))\n",
    "        x = torch.tensor(x)\n",
    "        y = model(x)\n",
    "        \n",
    "    return y #* Y_scale['acoustic'] + Y_mean['acoustic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_parameters(y_predicted):\n",
    "    # Number of time frames\n",
    "    T = y_predicted.shape[0]\n",
    "    \n",
    "    # Split acoustic features\n",
    "    mgc = y_predicted[:,:lf0_start_idx]\n",
    "    lf0 = y_predicted[:,lf0_start_idx:vuv_start_idx]\n",
    "    plt.plot(lf0[:, 0])\n",
    "    plt.show()\n",
    "    #lf0 = Y['acoustic']['train'][90][:, lf0_start_idx:vuv_start_idx]\n",
    "    #lf0 = np.zeros(lf0.shape)\n",
    "    vuv = y_predicted[:,vuv_start_idx]\n",
    "\n",
    "    plt.show()\n",
    "    bap = y_predicted[:,bap_start_idx:]\n",
    "    \n",
    "    # Perform MLPG\n",
    "    ty = \"acoustic\"\n",
    "    mgc_variances = np.tile(np.ones(Y_var[ty][:lf0_start_idx].shape), (T, 1))#np.tile(Y_var[ty][:lf0_start_idx], (T, 1))#\n",
    "    mgc = paramgen.mlpg(mgc, mgc_variances, windows)\n",
    "    lf0_variances = np.tile(np.ones(Y_var[ty][lf0_start_idx:vuv_start_idx].shape), (T,1))#np.tile(Y_var[ty][lf0_start_idx:vuv_start_idx], (T,1))#\n",
    "    lf0 = paramgen.mlpg(lf0, lf0_variances, windows)\n",
    "    bap_variances = np.tile(np.ones(Y_var[ty][bap_start_idx:].shape), (T, 1))#np.tile(Y_var[ty][bap_start_idx:], (T, 1))#\n",
    "    bap = paramgen.mlpg(bap, bap_variances, windows)\n",
    "    \n",
    "    return mgc, lf0, vuv, bap\n",
    "def gen_waveform(y_predicted, do_postfilter=False):  \n",
    "    y_predicted = trim_zeros_frames(y_predicted)\n",
    "        \n",
    "    # Generate parameters and split streams\n",
    "    mgc, lf0, vuv, bap = gen_parameters(y_predicted)\n",
    "    \n",
    "    if do_postfilter:\n",
    "        mgc = merlin_post_filter(mgc, alpha)\n",
    "        \n",
    "    spectrogram = pysptk.mc2sp(mgc, fftlen=fftlen, alpha=alpha)\n",
    "    aperiodicity = pyworld.decode_aperiodicity(bap.astype(np.float64), fs, fftlen)\n",
    "    f0 = lf0.copy()\n",
    "    f0[vuv < 0.5] = 0\n",
    "    f0[np.nonzero(f0)] = np.exp(f0[np.nonzero(f0)])\n",
    "    \n",
    "    generated_waveform = pyworld.synthesize(f0.flatten().astype(np.float64),\n",
    "                                            spectrogram.astype(np.float64),\n",
    "                                            aperiodicity.astype(np.float64),\n",
    "                                            fs, frame_period)\n",
    "    return generated_waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthesize(index, model=model, train=False):\n",
    "    model.eval()\n",
    "    y = recon(index, model=model, train=train)\n",
    "    IPython.display.display(Audio(gen_waveform(y.view(-1, 199).detach().numpy(), True), rate=fs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.display(Audio(gen_waveform(Y['acoustic']['test'][3], True), rate=fs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(A, B) :\n",
    "    return np.sqrt((np.square(A - B)).mean())\n",
    "\n",
    "def calc_lf0_rmse(natural, generated, lf0_idx=lf0_start_idx, vuv_idx=vuv_start_idx):\n",
    "    idx = (natural[:, vuv_idx] * (generated[:, vuv_idx] >= 0.5)).astype(bool)\n",
    "    return rmse(natural[idx, lf0_idx], generated[idx, lf0_idx]) * 1200 / np.log(2)  # unit: [cent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthesize(101, train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f0_error = 0\n",
    "for i in tqdm(range(len(x_test))):\n",
    "    y = recon(i).view(-1, 199).numpy()\n",
    "    y_true = Y['acoustic']['test'][i]\n",
    "    f0_error += calc_lf0_rmse(y, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f0_error / 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyRNN(nn.Module):\n",
    "    def __init__(self, D_in, H, D_out, num_layers=1, bidirectional=True):\n",
    "        super(MyRNN, self).__init__()\n",
    "        self.hidden_dim = H\n",
    "        self.num_layers = num_layers\n",
    "        self.num_direction =  2 if bidirectional else 1\n",
    "        self.lstm = nn.LSTM(D_in, H, num_layers, bidirectional=bidirectional, batch_first=True)\n",
    "        self.hidden2out = nn.Linear(self.num_direction*self.hidden_dim, D_out)\n",
    "        \n",
    "    def init_hidden(self, batch_size):\n",
    "        h, c = (Variable(torch.zeros(self.num_layers * self.num_direction, batch_size, self.hidden_dim)), \n",
    "                Variable(torch.zeros(self.num_layers * self.num_direction, batch_size, self.hidden_dim)))\n",
    "        return h,c\n",
    "\n",
    "    def forward(self, sequence):\n",
    "        h, c = self.init_hidden(batch_size=1)\n",
    "        output, (h, c) = self.lstm(sequence.view(1, -1, 535), (h, c))\n",
    "        output = self.hidden2out(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hidden_layers = 3\n",
    "hidden_size = 512\n",
    "batch_size = 8\n",
    "n_workers = 2\n",
    "pin_memory = True\n",
    "nepoch = 25\n",
    "lr = 0.002\n",
    "weight_decay = 1e-6\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(use_cuda)\n",
    "model_no_fc = MyRNN(X['acoustic'][\"train\"][0].shape[-1],\n",
    "                            hidden_size, Y['acoustic'][\"train\"][0].shape[-1],\n",
    "                            num_hidden_layers, bidirectional=True)\n",
    "model_no_fc.load_state_dict(torch.load('speechsynthesis_models/acoustic_state_dict', map_location=torch.device('cpu') ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthesize(0, model=model_no_fc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
