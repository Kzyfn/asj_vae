{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import expanduser, join\n",
    "\n",
    "\n",
    "import sys\n",
    "\n",
    "import time\n",
    "\n",
    "from nnmnkwii.datasets import FileDataSource, FileSourceDataset\n",
    "from nnmnkwii.datasets import PaddedFileSourceDataset, MemoryCacheDataset  # これはなに？\n",
    "from nnmnkwii.preprocessing import trim_zeros_frames, remove_zeros_frames\n",
    "from nnmnkwii.preprocessing import minmax, meanvar, minmax_scale, scale\n",
    "from nnmnkwii import paramgen\n",
    "from nnmnkwii.io import hts\n",
    "from nnmnkwii.frontend import merlin as fe\n",
    "from nnmnkwii.postfilters import merlin_post_filter\n",
    "\n",
    "from os.path import join, expanduser, basename, splitext, basename, exists\n",
    "import os\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "from scipy.io import wavfile\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pyworld\n",
    "import pysptk\n",
    "import librosa\n",
    "import librosa.display\n",
    "import IPython\n",
    "from IPython.display import Audio\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data as data_utils\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from tqdm import tnrange, tqdm\n",
    "from torch import optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mgc_dim = 180  # メルケプストラム次数　？？\n",
    "lf0_dim = 3  # 対数fo　？？ なんで次元が３？\n",
    "vuv_dim = 1  # 無声or 有声フラグ　？？\n",
    "bap_dim = 15  # 発話ごと非周期成分　？？\n",
    "\n",
    "duration_linguistic_dim = 438  # question_jp.hed で、ラベルに対する言語特徴量をルールベースで記述してる\n",
    "acoustic_linguisic_dim = 442  # 上のやつ+frame_features とは？？\n",
    "duration_dim = 1\n",
    "acoustic_dim = mgc_dim + lf0_dim + vuv_dim + bap_dim  # aoustice modelで求めたいもの\n",
    "\n",
    "fs = 48000\n",
    "frame_period = 5\n",
    "fftlen = pyworld.get_cheaptrick_fft_size(fs)\n",
    "alpha = pysptk.util.mcepalpha(fs)\n",
    "hop_length = int(0.001 * frame_period * fs)\n",
    "\n",
    "mgc_start_idx = 0\n",
    "lf0_start_idx = 180\n",
    "vuv_start_idx = 183\n",
    "bap_start_idx = 184\n",
    "\n",
    "windows = [\n",
    "    (0, 0, np.array([1.0])),\n",
    "    (1, 1, np.array([-0.5, 0.0, 0.5])),\n",
    "    (1, 1, np.array([1.0, -2.0, 1.0])),\n",
    "]\n",
    "\n",
    "use_phone_alignment = True\n",
    "acoustic_subphone_features = \"coarse_coding\" if use_phone_alignment else \"full\"  # とは？\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acoustic_linguisic_dim_model = 443\n",
    "class Rnn(nn.Module):\n",
    "    def __init__(self, bidirectional=True, num_layers=2):\n",
    "        super(Rnn, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.num_direction = 2 if bidirectional else 1\n",
    "        ##ここまでエンコーダ\n",
    "\n",
    "        self.fc11 = nn.Linear(\n",
    "            acoustic_linguisic_dim_model, acoustic_linguisic_dim_model\n",
    "        )\n",
    "\n",
    "        self.lstm2 = nn.LSTM(\n",
    "            acoustic_linguisic_dim_model,\n",
    "            512,\n",
    "            num_layers,\n",
    "            bidirectional=bidirectional,\n",
    "            dropout=0.15,\n",
    "        )\n",
    "        self.fc3 = nn.Linear(self.num_direction * 512, acoustic_dim)\n",
    "\n",
    "    def decode(self, linguistic_features):\n",
    "        x = self.fc11(linguistic_features.view(linguistic_features.size()[0], 1, -1))\n",
    "        x = F.relu(x)\n",
    "        h3, (h, c) = self.lstm2(x)\n",
    "        h3 = F.relu(h3)\n",
    "\n",
    "        return self.fc3(h3)  # torch.sigmoid(self.fc3(h3))\n",
    "\n",
    "    def forward(self, linguistic_features):\n",
    "\n",
    "        return self.decode(linguistic_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import VAE, VQVAE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from util import create_loader\n",
    "train_loader, valid_loader = create_loader(valid=True)\n",
    "train_loader, test_loader = create_loader(valid=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = VQVAE(num_layers=2, z_dim=1, num_class=4, repeat=1).to(device)\n",
    "#model.load_state_dict(torch.load('vqvae_2_1_4_93dim/2layers_zdim1_nc4/vqvae_model_30.pth', map_location=torch.device('cpu')))\n",
    "model.load_state_dict(torch.load('vqvae_2_1_4/vqvae_model_f0.pth', map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basemodel = torch.load('baseline_lower/baseline_lower', map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_l_labels_train = []\n",
    "h_l_labels_test = []\n",
    "h_l_labels_valid = []\n",
    "for i in range(5000):\n",
    "    h_l_label = np.loadtxt(\"./data/basic5000/accents/accents_\" + \"0\" * (4 - len(str(i + 1))) + str(i + 1) + \".csv\",)\n",
    "    if (i-1) % 20 == 0:\n",
    "        h_l_labels_test.append(h_l_label)\n",
    "    elif i % 20 == 0:\n",
    "        h_l_labels_valid.append(h_l_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recon(index, model=model, z0=False, valid=False, lh=False, verbose=True):\n",
    "    with torch.no_grad():\n",
    "        tmp = []\n",
    "        data = test_loader[index]\n",
    "        if valid:\n",
    "            data = valid_loader[index]\n",
    "        for j in range(2):\n",
    "            tmp.append(torch.from_numpy(data[j]).float().to(device))\\\n",
    "            \n",
    "        h_l_label = h_l_labels_test[index] if not valid else h_l_labels_valid[index]\n",
    "            \n",
    "        if z0:\n",
    "            y = model.decode(torch.tensor([[1]*1]*data[2].shape[0]), tmp[0], data[2])\n",
    "            return y\n",
    "        elif lh:\n",
    "            y = model.decode(torch.from_numpy(h_l_label*28 - 14).float(), tmp[0], data[2])\n",
    "            return y\n",
    "        y, mu, logvar = model(tmp[0], tmp[1], data[2], 0)\n",
    "        #print(mu.size())\n",
    "        #print(h_l_labels_test[index].shape)\n",
    "        \n",
    "        if verbose:\n",
    "            xlis = ['MO', 'KU', 'YO', 'O(U)', 'BI', 'TE', 'I', 'SE', 'N', 'KA', 'I', 'DA', 'N', 'WA',  'NA', 'N', 'NO', 'SHI'\n",
    "                   , 'N', 'TE', 'N', 'MO', 'NA', 'I', 'MA', 'MA', 'SHU', 'U', 'RYO', 'O(U)', 'SHI', 'MA', 'SHI', 'TA']\n",
    "            plt.figure(figsize=(20, 6))\n",
    "\n",
    "            plt.plot( mu.view(-1).cpu().numpy())\n",
    "            plt.plot( h_l_label*28 - 14)\n",
    "            plt.xticks(list(range(len(h_l_label))), xlis)\n",
    "\n",
    "\n",
    "            plt.show()\n",
    "            print(np.corrcoef([mu.view(-1).cpu().numpy(), h_l_label])[0][1])\n",
    "        \n",
    "    return y# mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthesize(index, model=model, verbose=True, z0=False, valid=False, lh=False):\n",
    "    y = recon(index, model=model, z0=z0, valid=valid, lh=lh)\n",
    "    y_base = test_loader[index][1].copy()#basemodel(torch.from_numpy(test_loader[index][0])).detach().cpu().numpy().reshape(-1, 199) if not valid else basemodel(torch.from_numpy(valid_loader[index][0])).detach().cpu().numpy().reshape(-1, 199)\n",
    "    y_base[:, lf0_start_idx] = y.cpu().numpy().reshape(-1)\n",
    "    y_base[:, lf0_start_idx+1:lf0_start_idx+3] = 0\n",
    "    if verbose:\n",
    "        IPython.display.display(Audio(gen_waveform(y_base, True), rate=fs))\n",
    "    else:\n",
    "        return y.cpu().numpy().reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "y_stats = pd.read_csv('data/y_stats.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_parameters(y_predicted, verbose=True):\n",
    "    # Number of time frames\n",
    "    T = y_predicted.shape[0]\n",
    "    \n",
    "    # Split acoustic features\n",
    "    mgc = y_predicted[:,:lf0_start_idx]\n",
    "    lf0 = y_predicted[:,lf0_start_idx:vuv_start_idx]\n",
    "    if verbose:\n",
    "        plt.plot(lf0[:, 0])\n",
    "        plt.show()\n",
    "    #lf0 = Y['acoustic']['train'][90][:, lf0_start_idx:vuv_start_idx]\n",
    "    #lf0 = np.zeros(lf0.shape)\n",
    "    vuv = y_predicted[:,vuv_start_idx]\n",
    "\n",
    "    plt.show()\n",
    "    bap = y_predicted[:,bap_start_idx:]\n",
    "    \n",
    "    # Perform MLPG\n",
    "    ty = \"acoustic\"\n",
    "    mgc_variances = np.tile(y_stats['var'][:lf0_start_idx], (T, 1))#np.tile(np.ones(Y_var[ty][:lf0_start_idx].shape), (T, 1))#\n",
    "    mgc = paramgen.mlpg(mgc, mgc_variances, windows)\n",
    "    lf0_variances = np.tile(y_stats['var'][lf0_start_idx:vuv_start_idx], (T,1))#np.tile(np.ones(Y_var[ty][lf0_start_idx:vuv_start_idx].shape), (T,1))#\n",
    "    lf0 = paramgen.mlpg(lf0, lf0_variances, windows)\n",
    "    bap_variances = np.tile(y_stats['var'][bap_start_idx:], (T, 1))#np.tile(np.ones(Y_var[ty][bap_start_idx:].shape), (T, 1))#\n",
    "    bap = paramgen.mlpg(bap, bap_variances, windows)\n",
    "    \n",
    "    return mgc, lf0, vuv, bap\n",
    "def gen_waveform(y_predicted, do_postfilter=False, verbose=True):  \n",
    "    y_predicted = trim_zeros_frames(y_predicted)\n",
    "        \n",
    "    # Generate parameters and split streams\n",
    "    mgc, lf0, vuv, bap = gen_parameters(y_predicted, verbose=verbose)\n",
    "    \n",
    "    if do_postfilter:\n",
    "        mgc = merlin_post_filter(mgc, alpha)\n",
    "        \n",
    "    spectrogram = pysptk.mc2sp(mgc, fftlen=fftlen, alpha=alpha)\n",
    "    aperiodicity = pyworld.decode_aperiodicity(bap.astype(np.float64), fs, fftlen)\n",
    "    f0 = lf0.copy()\n",
    "    f0[vuv < 0.5] = 0\n",
    "    f0[np.nonzero(f0)] = np.exp(f0[np.nonzero(f0)])\n",
    "    \n",
    "    generated_waveform = pyworld.synthesize(f0.flatten().astype(np.float64),\n",
    "                                            spectrogram.astype(np.float64),\n",
    "                                            aperiodicity.astype(np.float64),\n",
    "                                            fs, frame_period)\n",
    "    return generated_waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.quantized_vectors.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "index = 0\n",
    "valid=False\n",
    "synthesize(index, valid=valid)\n",
    "synthesize(index, z0=True, valid=valid)\n",
    "synthesize(index, valid=valid, lh=True)\n",
    "IPython.display.display(Audio(gen_waveform(test_loader[index][1], True), rate=fs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(A, B) :\n",
    "    return np.sqrt((np.square(A - B)).mean())\n",
    "\n",
    "def calc_lf0_rmse(natural, generated, lf0_idx=lf0_start_idx, vuv_idx=vuv_start_idx):\n",
    "    idx = (natural[:, vuv_idx]).astype(bool)\n",
    "    return rmse(natural[idx, lf0_idx], generated[idx]) * 1200 / np.log(2)  # unit: [cent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f0_e = 0\n",
    "for i, data in tqdm(enumerate(test_loader)):\n",
    "    with torch.no_grad():\n",
    "        pred_y = recon(i, z0=True).cpu().numpy().reshape(-1)\n",
    "        f0_e += calc_lf0_rmse(data[1], pred_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f0_e/250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f0_e = 0\n",
    "savedir = '../../disk/asj_vae/data/basic5000/generated/vqvae'\n",
    "for i, data in tqdm(enumerate(test_loader)):\n",
    "    with torch.no_grad():\n",
    "        pred_y = recon(i,  verbose=False).cpu().numpy().reshape(-1)\n",
    "        \n",
    "        y_base = data[1].copy()\n",
    "        y_base[:, lf0_start_idx] = pred_y\n",
    "        waveform = gen_waveform(y_base, True, verbose=False)\n",
    "        wavfile.write(join(savedir, 'BASIC5000_{}{}.wav'.format('0'*(4-len(str(i+1))), i+1)), rate=fs, data=waveform.astype(np.int16))\n",
    "        f0_e += calc_lf0_rmse(data[1], pred_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f0_e / 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f0_e = 0\n",
    "savedir = '../../disk/asj_vae/data/basic5000/generated/vqvae_z0'\n",
    "for i, data in tqdm(enumerate(test_loader)):\n",
    "    with torch.no_grad():\n",
    "        pred_y = recon(i, z0=True).cpu().numpy().reshape(-1)\n",
    "        \n",
    "        y_base = data[1].copy()\n",
    "        y_base[:, lf0_start_idx] = pred_y\n",
    "        wavfile.write(join(savedir, 'BASIC5000_{}{}_z0.wav'.format('0'*(4-len(str(i+1))), i+1)), rate=fs, data=gen_waveform(y_base, True, verbose=False).astype(np.int16))\n",
    "        f0_e += calc_lf0_rmse(data[1], pred_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f0_e / 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f0_e = 0\n",
    "savedir = '../../disk/asj_vae/data/basic5000/generated/vqvae_lh'\n",
    "for i, data in tqdm(enumerate(test_loader)):\n",
    "    with torch.no_grad():\n",
    "        pred_y = recon(i,lh=True).cpu().numpy().reshape(-1)\n",
    "        \n",
    "        y_base = data[1].copy()\n",
    "        y_base[:, lf0_start_idx] = pred_y\n",
    "        wavfile.write(join(savedir, 'BASIC5000_{}{}_lh.wav'.format('0'*(4-len(str(i+1))), i+1)), rate=fs, data=gen_waveform(y_base, True, verbose=False))\n",
    "        f0_e += calc_lf0_rmse(data[1], pred_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f0_e = 0\n",
    "for i, data in tqdm(enumerate(test_loader)):\n",
    "    with torch.no_grad():\n",
    "        pred_y = recon(i, lh=True).cpu().numpy().reshape(-1)\n",
    "        f0_e += calc_lf0_rmse(data[1], pred_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f0_e / 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
